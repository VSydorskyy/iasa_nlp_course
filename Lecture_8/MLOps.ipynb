{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57610618",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. [Introduction. What is MLOps?](#Intro)\n",
    "2. [Model Compression](#Compression)\n",
    "3. [Deployment. Simple Approach. BentoML](#BentoML)\n",
    "4. [Deployment. Advanced Approach. Custom Services](#Custom)\n",
    "5. [Homework](#Homework)\n",
    "\n",
    "<a id='Intro'></a>\n",
    "# Introduction. What is MLOps?\n",
    "MLOps stands for Machine Learning Operations. MLOps is a core function of Machine Learning engineering, focused on streamlining the process of taking machine learning models to production, and then maintaining and monitoring them. MLOps is a collaborative function, often comprising data scientists, devops engineers, and IT. <br>\n",
    "![](https://cms.databricks.com/sites/default/files/inline-images/mlops-components.png) <br>\n",
    "In this lecture our main focus will be on model inference / model deployments steps.\n",
    "\n",
    "<a id='Compression'></a>\n",
    "# Model Compression\n",
    "The primary benefit of compression involves reduced compute costs during inference: The computational resource reduction is the primary motivator for performing model compression. Model compression reduces CPU/GPU time, memory usage, and disk storage. It can make a model suitable for production that would have previously been too expensive, too slow, or too large. <br>\n",
    "\n",
    "Even when it’s beneficial, compression is not free. Costs of implementing it include:\n",
    "\n",
    "- Increased deployment complexity: After implementing various model compression techniques there is more to keep track of, namely the original trained model and the compressed models. We must choose the model to deploy and spend time making this choice.\n",
    "- Decreased accuracy: Some model compression techniques result in a loss of accuracy (however this is measured). This cost has an obvious counterpart in that the benefits of the model compression technique may outweigh the accuracy loss.\n",
    "- Compute cost: While model compression reduces the compute resources required for inference, the compression itself may be computationally expensive to perform. Notably, distillation introduces an additional iterative training step.\n",
    "- Your time: Adding a step to the lifecycle requires an investment of your time.\n",
    "\n",
    "> **TODO**: You can read more about compression [here](https://medium.com/data-science-at-microsoft/model-compression-and-optimization-why-think-bigger-when-you-can-think-smaller-216ec096f68b).\n",
    "\n",
    "## ONNX conversion and ONNX Runtime\n",
    "ONNX is an open format that is used to represent various Machine Learning models. It works by defining a common set of operators and a common file format to enable data scientists to use models in a wide variety of frameworks. The conversion process for natural language models from (insert your favorite neural network library here) to ONNX additionally functions as a model compression technique. This is because the operators defined by ONNX have been optimized for specific types of hardware, resulting in slightly smaller models.<br>\n",
    "\n",
    "\n",
    "The true utility of ONNX comes in the form of the ONNX Runtime backend. One of the optimizations with the most impact that ONNX Runtime implements is the capacity to “fuse” operations and activations within a model. The result of this fusion is a significant reduction in memory footprint and calculations per inference. For popular NLP model families, there exists customized logic to identify the operations within the models that can be fused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19309b6f",
   "metadata": {},
   "source": [
    "pip install onnx==1.14.1\n",
    "pip install onnxruntime\n",
    "pip install optimum[onnxruntime]==1.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e85940c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15738f984cf948cf88922c889d332430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491e93bbb9b74d9393b5b8344097ef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ac2f9dba66402a9ca92846ada5c0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4637b5d5c74aa3a9749a7551fa2c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c680bc9d12e453f817048a13b262916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c1dbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "onnx_path = './bert_base_uncased.onnx'\n",
    "\n",
    "dummy_input = tokenizer('here is the sample text for the dummy input', return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input['input_ids'].to(device), dummy_input['token_type_ids'].to(device), dummy_input['attention_mask'].to(device)),\n",
    "    f=onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size'},\n",
    "        'token_type_ids': {0: 'batch_size'},\n",
    "        'attention_mask': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd8975c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Inference Time: 3.77553391456604 seconds\n",
      "ONNX Model Inference Time: 2.408735990524292 seconds\n",
      "Inference acceleration is 1.57x times\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The road whispers secrets to those who travel without direction.\",\n",
    "    \"In the heart of the city, the jazz soul dances with shadows.\",\n",
    "    \"Under the moon's gaze, we found stories waiting in every corner.\",\n",
    "    \"The rhythm of the night was our compass through uncharted dreams.\",\n",
    "    \"With each mile, the horizon whispered promises of freedom.\",\n",
    "    \"In the silence of the mountains, our laughter echoed like ancient songs.\",\n",
    "    \"We were nomads of the twilight, seeking truth in the stars.\",\n",
    "    \"Every sunset was a painting, a masterpiece of our wanderlust.\",\n",
    "    \"The neon lights flickered, writing poetry in the dark.\",\n",
    "    \"Our conversations were a patchwork of memories and musings.\",\n",
    "    \"In the arms of the wilderness, we found our untamed spirits.\",\n",
    "    \"The city slept, but we walked its dreams.\",\n",
    "    \"With the dawn came clarity, like the first breath of a new world.\",\n",
    "    \"Our hearts beat to the rhythm of the train's song.\",\n",
    "    \"In the quiet cafes, we sipped on stories and coffee.\",\n",
    "    \"The road was a canvas, and our journey, its art.\",\n",
    "    \"Night skies told tales older than time in their starry script.\",\n",
    "    \"We found solace in the symphony of the wind and waves.\",\n",
    "    \"Each town held a secret, whispered in the rustling leaves.\",\n",
    "    \"We danced with shadows, embracing the mystery of the night.\",\n",
    "    \"The mountains stood as guardians of our deepest thoughts.\",\n",
    "    \"In the flicker of the campfire, our dreams danced freely.\",\n",
    "    \"The desert's vastness echoed our longing for the unknown.\",\n",
    "    \"Our path was lit by the hopes of a thousand adventures.\",\n",
    "    \"In the depths of the forest, time stood still, a silent witness.\",\n",
    "    \"We followed the river's song, meandering through forgotten lands.\",\n",
    "    \"The city's heartbeat was a melody of chaos and beauty.\",\n",
    "    \"Under the starlit sky, our souls whispered tales of old.\",\n",
    "    \"The open road was our teacher, life its lesson.\",\n",
    "    \"In every journey's end, a new story was born.\",\n",
    "    \"We were seekers of the dawn, chasing the first light.\",\n",
    "    \"The rain's rhythm spoke of journeys yet to come.\",\n",
    "    \"Our map was drawn in dreams and detours.\",\n",
    "    \"In the twilight, the world seemed to pause, listening to our footsteps.\",\n",
    "    \"We wandered through the pages of the earth, writing our story.\",\n",
    "    \"The ocean's vastness mirrored our boundless curiosity.\",\n",
    "    \"In the stillness of the night, every star held a wish.\",\n",
    "    \"Our laughter was the soundtrack of endless roads.\",\n",
    "    \"We found poetry in the ordinary, magic in the mundane.\",\n",
    "    \"Every mile traveled was a verse in our epic.\",\n",
    "    \"The whispers of the wind were our guide through the unknown.\",\n",
    "    \"In the heart of the forest, we spoke the language of the wild.\",\n",
    "    \"The city at dawn was a canvas of hushed possibilities.\",\n",
    "    \"Our journey was a mosaic of moments, each a priceless gem.\",\n",
    "    \"With each setting sun, our stories grew richer.\",\n",
    "    \"The mountains called to us, their peaks like beckoning fingers.\",\n",
    "    \"In the quiet of the countryside, our thoughts found voice.\",\n",
    "    \"We were pilgrims of the moonlight, worshiping the night.\",\n",
    "    \"The road's end was not a destination, but a new beginning.\",\n",
    "    \"In the labyrinth of streets, we found pieces of ourselves.\"\n",
    "]\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "end_time = time.time()\n",
    "\n",
    "original_model_time = end_time - start_time\n",
    "print(f\"Original Model Inference Time: {original_model_time} seconds\")\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"bert_base_uncased.onnx\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time = end_time - start_time\n",
    "print(f\"ONNX Model Inference Time: {onnx_model_time} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e782b8",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Our next approach, quantization, is the process of mapping values from a large set to a smaller set. Rounding and truncation are both basic examples of quantization but aren’t how quantization manifests in the realm of neural networks.\n",
    "\n",
    "\n",
    "Neural nets, in most default configurations, have weights stored as 32-bit floating point numbers (fp32). Operations with fp32 numbers are expensive and most hardware is not optimized to compute with them.\n",
    "\n",
    "\n",
    "The most common quantization process takes fp32 numbers and reduces them to 8-bit integers (int8). The result is a model with a quarter the size that can perform inference at nearly four times the original speed. These benefits are at the cost of a loss in precision in the output of the model. Whether this loss in precision affects the target metric for the model is task and model dependent. Typically, when models have discrete outputs, such as identification of a handwritten digit, this precision loss has less effect.\n",
    "\n",
    "\n",
    "This form of quantization comes with a catch. Moving from fp32 to int8 is most beneficial for models inferencing on the CPU.\n",
    "\n",
    "![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png)\n",
    "![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/8-bit-signed-integer-quantization.png)\n",
    "\n",
    "> **TODO**: You can read more about the exact process details [here](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/) and get more practical transformers examples [here](https://github.com/ELS-RD/transformer-deploy/blob/main/demo/quantization/quantization_end_to_end.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220ab183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.0/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.1/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.2/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.3/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.4/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.5/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.6/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.7/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.8/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.9/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.10/attention/self/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/bert/encoder/layer.11/attention/self/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\"bert_base_uncased.onnx\", \"bert_base_uncased_quant.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e92d5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Quantized Model Inference Time: 2.420318365097046 seconds\n",
      "Inference acceleration is 1.56x times\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"bert_base_uncased_quant.onnx\")\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time_quant = end_time - start_time\n",
    "print(f\"ONNX Quantized Model Inference Time: {onnx_model_time_quant} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time_quant, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eabde5",
   "metadata": {},
   "source": [
    "## Distillation\n",
    "![](https://i0.wp.com/www.merchantnavydecoded.com/wp-content/uploads/2023/06/steam-egine-26.png?fit=1140%2C570&ssl=1)\n",
    "Distillation is one of the most powerful approaches when it comes to model compression. Implementing a state-of-the-art distillation process can cut your model size down by a factor of seven, increase inference speeds by a factor of ten, and have almost no effect on the model’s accuracy metric (e.g. distilBERT tinyBERT).\n",
    "\n",
    "\n",
    "Here is more good news: Distillation is still fairly young! There are likely many improvements to come. Now for some bad news: Distillation is still fairly young! This means that the process is not yet widely implemented in standard libraries. Research code does exist that can be used to distill various model architectures (such as BERT, GPT2, and BART), though to implement distillation on a custom model it is necessary to understand the full process.\n",
    "\n",
    "> **TODO**: You can read more about the exact process details [here](https://medium.com/p/dd4973dbc764)\n",
    "\n",
    "Teacher Student networks — How do they exactly work?\n",
    "- Train the Teacher Network : The highly complex teacher network is first trained separately using the complete dataset. This step requires high computational performance and thus can only be done offline (on high performing GPUs).\n",
    "- Establish Correspondence : While designing a student network, a correspondence needs to be established between intermediate outputs of the student network and the teacher network. This correspondence can involve directly passing the output of a layer in the teacher network to the student network, or performing some data augmentation before passing it to the student network. The way the knowledge of the good answers is transferred to the Student is through the loss function. Essentially, we want to train the Student so that it mimics the same distribution that the Teacher provides. To do this, we must also understand what the Student outputs are before it is even trained. This measurement is called the Kullback-Leibler, or KL, divergence. This approximates the work it takes to turn the red curve into the blue curve. The result is a loss function that has a term measuring the KL divergence between the Student distribution and the Teacher distribution.\n",
    "\n",
    "![](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Knowledge-Distillation_4.png?resize=900%2C356&ssl=1)\n",
    "- Forward Pass through the Teacher network : Pass the data through the teacher network to get all intermediate outputs and then apply data augmentation (if any) to the same.\n",
    "- Backpropagation through the Student Network : Now use the outputs from the teacher network and the correspondence relation to backpropagate error in the student network, so that the student network can learn to replicate the behavior of the teacher network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2ff9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4889048fdc4501852728b6a10d8d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644ff84029a54a3ab885e6c297a390e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1163dc4b61f4cfe8d7a91523e5d6187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e927595f1cbe4e64b683579a789fc744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893896f23e6e411dbeb40c5c7ffa0ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Model Inference Time: 1.9083404541015625 seconds\n",
      "Inference acceleration is 1.98x times\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "end_time = time.time()\n",
    "\n",
    "distilled_model_time = end_time - start_time\n",
    "print(f\"Distilled Model Inference Time: {distilled_model_time} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/distilled_model_time, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8266",
   "metadata": {},
   "source": [
    "## Distillation + ONNX + Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e8d00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abazdyrev/anaconda3/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.0/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.1/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.2/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.3/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.4/attention/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/distilbert/transformer/layer.5/attention/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "onnx_path = './distilbert_base_uncased.onnx'\n",
    "\n",
    "dummy_input = tokenizer('here is the sample text for the dummy input', return_tensors=\"pt\", max_length=50, padding='max_length')\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input['input_ids'].to(device), dummy_input['attention_mask'].to(device)),\n",
    "    f=onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size'},\n",
    "        'attention_mask': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "quantize_dynamic(onnx_path, './distilbert_base_uncased_quant.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ca2338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Distilled Quantized Model Inference Time: 2.420318365097046 seconds\n",
      "Inference acceleration is 4.08x times\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "session = ort.InferenceSession('distilbert_base_uncased_quant.onnx')\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "for text in texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", max_length=50, padding='max_length')\n",
    "    inputs_onnx = {k: v for k, v in inputs.items()}\n",
    "    outputs = session.run(None, inputs_onnx)\n",
    "end_time = time.time()\n",
    "\n",
    "onnx_model_time_quant_distilled = end_time - start_time\n",
    "print(f\"ONNX Distilled Quantized Model Inference Time: {onnx_model_time_quant} seconds\")\n",
    "print(f\"Inference acceleration is {round(original_model_time/onnx_model_time_quant_distilled, 2)}x times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da320e51",
   "metadata": {},
   "source": [
    "<a id='BentoML'></a>\n",
    "# Deployment. Simple Approach. BentoML\n",
    "\n",
    "ML model deployment is the process of integrating a trained ML model into an existing production environment to make practical, actionable decisions based on new data. It's a crucial step in a machine learning project as it allows the model to provide real-world value. Here we will consider **Real-time Inference** - for applications requiring immediate feedback, models are deployed in an environment that supports real-time data processing with a simple implementation using BentoML.\n",
    "\n",
    "**BentoML** is designed for teams working to bring machine learning (ML) models into production in a reliable, scalable, and cost-efficient way. In particular, AI application developers can leverage BentoML to easily integrate state-of-the-art pre-trained models into their applications. By seamlessly bridging the gap between model creation and production deployment, BentoML promotes collaboration between developers and in-house data science teams.\n",
    "\n",
    "We will take a \n",
    "\n",
    "TODO read documentations and do some experiments with more advanced options [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4e0de-df73-4c29-8a5c-c178f66543b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96fd8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181dfda021e94f59ae343e62c53b0fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8a9ca6723c4f4e90d12a64daa7afea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc57f26ab5d24800862cc153d23266dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f186f28bcb94fb6ad6588b6968258a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Model(tag=\"text-classification-pipe:hxyntpedy6th4qqb\", path=\"/home/abazdyrev/bentoml/models/text-classification-pipe/hxyntpedy6th4qqb/\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bentoml\n",
    "import transformers\n",
    "\n",
    "pipe = transformers.pipeline(\"text-classification\", device='cpu')\n",
    "\n",
    "bentoml.transformers.save_model(\n",
    "  \"text-classification-pipe\",\n",
    "  pipe,\n",
    "  signatures={\n",
    "    \"__call__\": {\"batchable\": True}  # Enable dynamic batching for model\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfc92f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1m \u001b[0m\u001b[1mTag                    \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mModule              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mSize      \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mCreation Time      \u001b[0m\u001b[1m \u001b[0m\n",
      " text-classification-pi…  bentoml.transformers  256.35 MiB  2023-11-15 14:57:05 \n"
     ]
    }
   ],
   "source": [
    "!bentoml models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f26569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Environ for worker 0: set CPU thread count to 8\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Prometheus metrics for HTTP BentoServer from \"service.py:svc\" can be accessed at http://localhost:3000/metrics.\n",
      "2023-11-15T14:58:07+0000 [INFO] [cli] Starting production HTTP BentoServer from \"service.py:svc\" listening on http://0.0.0.0:3000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve bentoml_service.py:svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c29e5e-a795-45de-bcd7-bf7cbdec687b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"POSITIVE\",\"score\":0.9998418092727661}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://localhost:3000/classify', data=\"BentoML is awesome\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "646f63df-d8aa-4b43-941d-efbcc9103f60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"label\":\"NEGATIVE\",\"score\":0.999693751335144}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.post('http://localhost:3000/classify', data=\"ML deployment is a very complicated and awful process\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecf99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0904754-98d6-45a8-9e55-165763dfa8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa0e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
